# GDM-FLIP: Photographer Inspired Image Masking for Language-Image Model Pre-Training
![GDM-FLIP](./docs/GDM-FLIP.jpg)


# Abstract
We introduce a novel, straightforward, and effective technique for masking image patches in the training of the Vision-Language Model. Different from FLIP, which involves randomly masking and removing a significant portion of the image patches. This method employs a Gaussian Distribution to mask image patches (GDM-FLIP). This strategy is inspired by typical human photographic behavior, where subjects or objects are often centered within the image. GDM-FLIP incorporates this behavior by emphasizing the center of the image during training, enabling the model to efficiently learn the relationship between images and text without the need for complex computations. In comparative experiments on various pre-training datasets, GDM-FLIP outperformed FLIP across a range of downstream datasets and tasks. Furthermore, we conducted an analysis on different types of datasets to demonstrate GDM-FLIPâ€™s advantages across diverse data contexts.

# Method

**Figure: Random Masking (Left) vs. Gaussian Masking (Right)**

![Random Masking](./docs/random_mask_image.png) ![Gaussian Masking](./docs/gaussian_mask_image.png)

*In contrast to the random mask strategy, our approach retains a greater number of image patches at the center of the image. We set $\sigma_x$ and $\sigma_y$ as 0.20 for Formula 1.*



# Results and Pre-trained Models